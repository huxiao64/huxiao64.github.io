---
layout: post
title: 十分钟了解分布式计算:Petuum
description: Petuum是一个机器学习专用分布式计算框架，是CMU的Eric Xing教授带头的开源项目。本文介绍其架构，并基于文章 More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server，NIPS 2013 重点探讨其核心内容SSP协议。
category: tech
---

[Petuum](https://github.com/sailinglab/petuum)是一个机器学习专用分布式计算框架，是CMU的Eric Xing教授带头的开源项目。本文介绍其架构，并基于文章 More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server，NIPS 2013 重点探讨其核心内容SSP协议。
#####主要思想
Parameter server提供了一个易于读写Global模型参数的接口，而SSP协议允许distributed workers读写本地缓存中stale版本的参数（而不是每次都花大量时间时间等待central storage传回最新参数）。更进一步，通过限制参数的stale程度，SSP模型提供了机器学习算法的正确性保证。
![](http://images.cnitblog.com/blog/331825/201406/131853410775804.png)

#####Stale Synchronous Parallel (SSP)
2. 并行机器学习面临着两个挑战：集群本身的Unequal performance machines和网络通信上的Low bandwidth, High delay问题。集群越大，线性扩展的代价就越大，网络通信会占据时间开销的主要部分。![](http://images.cnitblog.com/blog/331825/201406/131850312174475.png)
3. BSP和Asynchronous协议各有缺点![](http://images.cnitblog.com/blog/331825/201406/131858180452356.png)![](http://images.cnitblog.com/blog/331825/201406/131859099675660.png)
3. SSP协议的好处在于，faster worker会遇到参数版本过于stale的问题，导致每一步迭代都需要网络通信，从而达到了平衡计算和网络通信时间开销的效果。![](http://images.cnitblog.com/blog/331825/201406/131850529839757.png)
4. Petuum提供了分布式共享global模型参数的接口，使得很容易可以将多线程版本算法修改为Petuum版本。![](http://images.cnitblog.com/blog/331825/201406/131856129831662.png)
3. SSP放宽一致性约束后，在恰当的参数下能算的更快。![](http://images.cnitblog.com/blog/331825/201406/131852388587778.png)但其中有个迭代次数和每次迭代耗时的trade off![](http://images.cnitblog.com/blog/331825/201406/131852069998245.png)
3. Asynchronous的问题在于，整体对参数的更新量delta_w=delta_w1+delta_w2+...(delta_wi表示单个worker i根据部分数据计算的参数更新量），delta_wi之间应该是不能跨迭代次数的(而SSP则是放宽了这种约束)，因此Asynchronous并没有收敛的保证。假如我们可以通过概率假设，证明有一个概率版本的收敛速率，如果有一个bound，提出reduce bound的方法，会很有意义。![](http://images.cnitblog.com/blog/331825/201406/131851474673847.png)
4. 对于非凸问题来说，BSP和SSP有可能收敛到的最优解不一样。对于非凸优化问题（比如说神经网络），有大量局部最优解，随机梯度下降（可以跳出局部最优解）比批量梯度下降效果要更好。LDA本身也是非凸优化问题，不过如果采用变分法就会目标函数变成凸优化。

#####Structure-aware dynamic scheduler (STRADS)
![](http://images.cnitblog.com/blog/331825/201406/131854075144728.png)
1. STRADS负责模型的并行，涉及到参数的partition。
2. LDA（主题参数，归属主题（混合概率），隐变量）和DL模型（分层参数）的参数具有天然的分块，可能会好做一些。

#####Fault tolerance
1. Petuum的Fault tolerance功能非常简单，通过在Parameter Sever上taking snapshots，将参数备份到持久化存储，而结点的故障恢复是没有支持的。

####杂谈
前两天去ICML看到了Eric Xing真人，他说之所以Petuum用C++而不是Scala等在Github上活跃的语言实现是因为这只是一个原型，GraphX的图并行太局限了无法处理真正的海量数据，且GraphX的点一致性模式没有收敛保证而Petuum的SSP是理论保证的。虽说文人相轻，但GraphX的model partition是否会遇到瓶颈，在海量数据中Petuum的正确性和效率是否会击败GraphX，还得让实际应用说话了。

### Resource
[Petuum: Source Code Read and Initial Test Result](http://yinxusen.github.io/blog/2014/01/17/petuum-source-code-read-and-initial-test-result/)

[How to Use Spark for ML Algorithms and Why ?](http://yinxusen.github.io/blog/2014/01/18/how-to-use-spark-for-ml-algorithms-and-why/)

### Publications
[1] [More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server](http://www.cs.cmu.edu/~epxing/papers/2013/SSPTable_NIPS2013.pdf) Qirong Ho, James Cipar, Henggang Cui, Jin Kyu Kim, Seunghak Lee, Phillip. B. Gibbons, Garth A. Gibson, Greg R. Ganger, Eric P. Xing.
Neural Information Processing Systems, 2013 (NIPS 2013)
[Slides](http://www.cs.cmu.edu/~qho/ssp_nips2013.pdf)

[More](http://petuum.org/research.html)

