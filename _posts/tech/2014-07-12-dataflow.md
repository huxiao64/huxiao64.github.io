---
layout: post
title: 十分钟了解分布式计算:Google Dataflow
description: Google声称内部已经抛弃Map-reduce几年了，并在今年的Google IO大会高调发布Cloud Dataflow系统。本文介绍了在这个新框架下如何进行分布式计算，并与现有基于Hadoop的Scalding,Summingbird(Twitter)以及Spark框架进行了对比。
category: tech
---
####介绍
Cloud Dataflow是一种构建、管理和优化复杂数据处理流水线的方法，集成了许多内部技术，如用于数据高效并行化处理的[Flume](http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf)和具有良好容错机制流处理的[MillWheel](http://research.google.com/pubs/pub41378.html)。Dataflow当前的API还只有Java版本（其实Flume本身是提供Java/C++/Python多种接口的）。

相比原生的map-reduce模型，Dataflow有几个优点：

1. 可以构建复杂的pipeline，在这不妨引用Google云平台的产品营销总监Brian Goldfarb的话
>Cloud Dataflow可以用于处理批量数据和流数据两种。在一个世界性事件（比如演讲当中的世界杯事件）中，实时分析上百万twitter数据。在流水线的一个部阶段责读取tweet，下一个阶段负责抽取标签。另一个阶段对tweet分类（基于情感，正面负面或者其他方面）。下一个阶段过滤关键词等等。相比之下，Map/Reduce这个用来处理大数据的较早模型，处理这种实时数据已经力不从心，而且也很难应用到这种很长很复杂的数据流水线上。

2. 不需手工配置和管理MapReduce集群。自动进行代码优化和资源调度，使得开发者的主要精力可以放在业务逻辑本身 
![](/images/dataflow/dataflow.png)

3. 支持从Batch到Streaming模式的无缝切换:
假设我们要根据用户在twitter上产生的内容，来实现一个hashtags自动补全的功能
><table style="font-family: Times;" border="1">
<caption>Example: Auto completing hashtags</caption>
<tbody>
<tr><th align="center">Prefix</th><th align="center">Suggestions</th></tr>
<tr>
<td>ar</td>
<td>#argentina, #arugularocks, #argylesocks</td>
</tr>
<tr>
<td>arg</td>
<td>#argentina, #argylesocks, #argonauts</td>
</tr>
<tr>
<td>arge</td>
<td>#argentina, #argentum, #argentine</td>
</tr>
</tbody>
</table>
![](/images/dataflow/pipeline0.png)
代码几乎和数据流一一对应，和单机程序的编写方式差别不大
![](/images/dataflow/pipeline1.png)
上述是批处理的做法
![](/images/dataflow/batch.png)
转化为streaming做法只需改动数据源。如果我们现在希望模型提供的是最新的热词，考虑数据的时效性，只需额外添加一行设置数据window的操作，比如说60min以前的数据我们就不要了
![](/images/dataflow/streaming.png)

4. Dashboard:
还可以在developer console中了解流水线中每个环节执行的情况，每个流程框基本对应着一行代码
![](/images/dataflow/dashboard.png)

5. 生态系统:
BigQuery作为存储系统是Dataflow的一个补充，经过Dataflow清洗和处理过的数据，可以在BigQuery中存下来，同时Dataflow也可以读取BigQuery以进行表连接等操作。如果想在Dataflow上使用一些开源资源（比如说Spark中的机器学习库），也是很方便的
![](/images/dataflow/eco.png)

为了配合Dataflow，[Google Cloud Platform](https://developers.google.com/cloud/)还为开发者提供了一系列工具，包括云保存，云调试，云追踪和云监控。

####比较
1. __Cascading/Twitter Scalding__：
1) 传统Map-reduce只能处理单一的流，而Dataflow可以构建整个pipeline，自动优化和调度，Dataflow乍一听感觉非常像Hadoop上的[Cascading](http://www.cascading.org/)(Java)/[Scalding](https://github.com/twitter/scalding/)(Scala)。
2) 它们的编程模型很像，Dataflow也可以很方便做本地测试，可以传一个模拟集合，在上面去迭代计算结果，这一点是传统Map-reduce望尘莫及的。
2. __Twitter Summingbird__：
而将批处理和流处理无缝连接的思想又听起来很像把Scalding和[Strom](https://github.com/nathanmarz/storm)无缝连接起来的twitter [summingbird](https://github.com/twitter/summingbird)(Scala).
3. __Spark__：
1) [Spark](https://spark.incubator.apache.org/)也有可以构建复杂的pipeline做一代码优化和任务调度的好处，但目前还需要程序员来配置资源分配。
2) Spark在设计分布式数据集API时，模拟了Scala集合的操作API，使得额外的语法学习成本比Dataflow要低。
3) 不过Dataflow并没有提内存计算的事儿，这一点跟Spark有着本质的区别，它把Spark作为一种很好的Open Source补充。
4) 分布式计算中除了Batch和Streaming，Graph也是一个重要的问题，Spark在这方面有GraphX，Dataflow在未来也会将处理Graph处理这块整合进去。

####参考
本文的内容主要基于官方资料

[Sneak peek: Google Cloud Dataflow, a Cloud-native data processing service](http://googlecloudplatform.blogspot.com/2014/06/sneak-peek-google-cloud-dataflow-a-cloud-native-data-processing-service.html)

[Google I/O 2014 - The dawn of "Fast Data"](http://www.youtube.com/watch?v=TnLiEWglqHk)(国内用户[下载](http://www.clipconverter.cc/download/-zyrPs6V/108992264/))

####链接
[Google Cloud Dataflow 简单理解](http://weibo.com/p/1001603730758610079518)

[Cloud Dataflow ：云计算时代的新计算模式](http://www.infoq.com/cn/news/2014/07/cloud-dataflow)

[Google Announces Cloud Dataflow Beta at Google I/O](http://www.infoq.com/news/2014/06/google-io-cloud-dataflow)

[Google Launches Cloud Dataflow, A Managed Data Processing Service](http://techcrunch.com/2014/06/25/google-launches-cloud-dataflow-a-managed-data-processing-service/)

[Mapreduce successor google cloud dataflow is a game changer for hadoop thunder](http://cloudtimes.org/2014/07/07/mapreduce-successor-google-cloud-dataflow-is-a-game-changer-for-hadoop-thunder/)

####论文
[FlumeJava: Easy, Efficient Data-Parallel Pipelines,PLDI,2010](http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf)

[MillWheel: Fault-Tolerant Stream Processing at Internet Scale,Very Large Data Bases (2013), pp. 734-746](http://research.google.com/pubs/pub41378.html)